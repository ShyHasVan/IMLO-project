{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQl-2CRVuZWS",
        "outputId": "c6a1e233-23a3-4c2a-a782-d707f4d90ee1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://thor.robots.ox.ac.uk/datasets/flowers-102/102flowers.tgz to data/flowers-102/102flowers.tgz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 99%|█████████▊| 339738624/344862509 [00:20<00:02, 1920225.87it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "custom_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "])\n",
        "\n",
        "training_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split= \"train\",\n",
        "    download=True,\n",
        "    transform= custom_transform,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "\n",
        "test_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split= \"test\",\n",
        "    download=True,\n",
        "    transform= custom_transform,\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True)\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "count=0\n",
        "for i, (X_Train, y_train) in enumerate(training_data):\n",
        "  count+= 1\n",
        "\n",
        "conv1 = nn.Conv2d(3, 18, 3, 1, 'same')\n",
        "conv2 = nn.Conv2d(18, 144, 3, 1, 'same')\n",
        "conv3 = nn.Conv2d(144, 360, 3, 1, 'same')\n",
        "x = F.relu(conv1(X_Train))\n",
        "\n",
        "x = F.max_pool2d(x, 2, 2)\n",
        "x.shape\n",
        "\n",
        "x = F.relu(conv2(x))\n",
        "\n",
        "x = F.max_pool2d(x, 2, 2)\n",
        "x = F.relu(conv3(x))\n",
        "\n",
        "x = F.max_pool2d(x, 2, 2)\n",
        "x.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2bp2699ulxj"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.conv1 = nn.Conv2d(3, 9, 3, 1, 'same')\n",
        "        self.conv2 = nn.Conv2d(9, 18, 3, 1, 'same')\n",
        "        #Fully Connected Layer\n",
        "        self.fc1 = nn.Linear(18*56*56, 200)\n",
        "        self.fc2 = nn.Linear(200, 150)\n",
        "        self.fc3 = nn.Linear(150, 102)\n",
        "\n",
        "    def forward(self, X):\n",
        "      x = self.flatten(X)\n",
        "      X = F.relu(self.conv1(X))\n",
        "      X = F.max_pool2d(X,2,2)\n",
        "      #Second Pass\n",
        "      X = F.relu(self.conv2(X))\n",
        "      X = F.max_pool2d(X,2,2)\n",
        "      #Fully Connected Layers\n",
        "      X = X.view(-1, 18*56*56)\n",
        "      X = F.relu(self.fc1(X))\n",
        "      X = F.relu(self.fc2(X))\n",
        "      X = self.fc3(X)\n",
        "      return F.log_softmax(X, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeYBtuEtEdnD"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(64)\n",
        "model  =  NeuralNetwork()\n",
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOyGjfpDSc85"
      },
      "outputs": [],
      "source": [
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    # Set the model to training mode - important for batch normalization and dropout layers\n",
        "    # Unnecessary in this situation but added for best practices\n",
        "    model.train()\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        # Compute prediction and loss\n",
        "        pred = model(X)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        if batch % 1 == 0:\n",
        "            loss, current = loss.item(), batch * batch_size + len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "\n",
        "def test_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    num_batches = len(dataloader)\n",
        "    with torch.no_grad():\n",
        "        for x, y in dataloader:\n",
        "            pred = model(x)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            _, predicted = torch.max(pred, 1)\n",
        "            correct += (predicted == y).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_ahimLFhhQ"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWZH7JsbPYvq",
        "outputId": "f5a32fc0-9306-4a27-be46-1b3b79d0d856"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "-------------------------------\n",
            "loss: 4.632421  [   64/ 1020]\n",
            "loss: 4.728790  [  128/ 1020]\n",
            "loss: 4.807528  [  192/ 1020]\n",
            "loss: 4.737648  [  256/ 1020]\n",
            "loss: 4.646730  [  320/ 1020]\n",
            "loss: 4.665675  [  384/ 1020]\n",
            "loss: 4.775331  [  448/ 1020]\n",
            "loss: 4.666566  [  512/ 1020]\n",
            "loss: 4.619850  [  576/ 1020]\n",
            "loss: 4.611726  [  640/ 1020]\n",
            "loss: 4.597502  [  704/ 1020]\n",
            "loss: 4.623504  [  768/ 1020]\n",
            "loss: 4.622926  [  832/ 1020]\n",
            "loss: 4.589792  [  896/ 1020]\n",
            "loss: 4.602981  [  960/ 1020]\n",
            "loss: 4.629015  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 2.1%, Avg loss: 4.585814 \n",
            "\n",
            "Epoch 2\n",
            "-------------------------------\n",
            "loss: 4.541603  [   64/ 1020]\n",
            "loss: 4.549908  [  128/ 1020]\n",
            "loss: 4.526780  [  192/ 1020]\n",
            "loss: 4.540767  [  256/ 1020]\n",
            "loss: 4.548572  [  320/ 1020]\n",
            "loss: 4.470823  [  384/ 1020]\n",
            "loss: 4.534067  [  448/ 1020]\n",
            "loss: 4.357556  [  512/ 1020]\n",
            "loss: 4.527840  [  576/ 1020]\n",
            "loss: 4.304918  [  640/ 1020]\n",
            "loss: 4.374347  [  704/ 1020]\n",
            "loss: 4.496682  [  768/ 1020]\n",
            "loss: 4.436764  [  832/ 1020]\n",
            "loss: 4.458569  [  896/ 1020]\n",
            "loss: 4.438516  [  960/ 1020]\n",
            "loss: 4.391532  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 2.8%, Avg loss: 4.388488 \n",
            "\n",
            "Epoch 3\n",
            "-------------------------------\n",
            "loss: 4.293872  [   64/ 1020]\n",
            "loss: 4.302397  [  128/ 1020]\n",
            "loss: 4.376930  [  192/ 1020]\n",
            "loss: 4.234460  [  256/ 1020]\n",
            "loss: 4.263458  [  320/ 1020]\n",
            "loss: 3.984767  [  384/ 1020]\n",
            "loss: 4.062659  [  448/ 1020]\n",
            "loss: 4.151090  [  512/ 1020]\n",
            "loss: 4.122560  [  576/ 1020]\n",
            "loss: 4.111074  [  640/ 1020]\n",
            "loss: 4.233528  [  704/ 1020]\n",
            "loss: 4.109105  [  768/ 1020]\n",
            "loss: 4.140545  [  832/ 1020]\n",
            "loss: 4.139225  [  896/ 1020]\n",
            "loss: 4.067676  [  960/ 1020]\n",
            "loss: 4.092090  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 3.4%, Avg loss: 4.210434 \n",
            "\n",
            "Epoch 4\n",
            "-------------------------------\n",
            "loss: 3.983991  [   64/ 1020]\n",
            "loss: 3.974523  [  128/ 1020]\n",
            "loss: 4.183474  [  192/ 1020]\n",
            "loss: 3.925042  [  256/ 1020]\n",
            "loss: 3.898952  [  320/ 1020]\n",
            "loss: 3.748171  [  384/ 1020]\n",
            "loss: 3.906982  [  448/ 1020]\n",
            "loss: 4.075375  [  512/ 1020]\n",
            "loss: 3.908340  [  576/ 1020]\n",
            "loss: 3.801360  [  640/ 1020]\n",
            "loss: 3.987624  [  704/ 1020]\n",
            "loss: 3.858117  [  768/ 1020]\n",
            "loss: 3.850214  [  832/ 1020]\n",
            "loss: 3.970411  [  896/ 1020]\n",
            "loss: 3.972312  [  960/ 1020]\n",
            "loss: 3.843767  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 6.0%, Avg loss: 4.029853 \n",
            "\n",
            "Epoch 5\n",
            "-------------------------------\n",
            "loss: 3.661981  [   64/ 1020]\n",
            "loss: 3.732137  [  128/ 1020]\n",
            "loss: 3.588350  [  192/ 1020]\n",
            "loss: 3.717437  [  256/ 1020]\n",
            "loss: 3.738578  [  320/ 1020]\n",
            "loss: 3.685113  [  384/ 1020]\n",
            "loss: 3.693251  [  448/ 1020]\n",
            "loss: 3.653510  [  512/ 1020]\n",
            "loss: 3.651818  [  576/ 1020]\n",
            "loss: 3.597482  [  640/ 1020]\n",
            "loss: 4.029201  [  704/ 1020]\n",
            "loss: 3.804196  [  768/ 1020]\n",
            "loss: 3.830206  [  832/ 1020]\n",
            "loss: 3.794099  [  896/ 1020]\n",
            "loss: 3.716051  [  960/ 1020]\n",
            "loss: 3.748359  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 6.9%, Avg loss: 3.935168 \n",
            "\n",
            "Epoch 6\n",
            "-------------------------------\n",
            "loss: 3.466146  [   64/ 1020]\n",
            "loss: 3.561247  [  128/ 1020]\n",
            "loss: 3.761217  [  192/ 1020]\n",
            "loss: 3.447231  [  256/ 1020]\n",
            "loss: 3.466115  [  320/ 1020]\n",
            "loss: 3.569676  [  384/ 1020]\n",
            "loss: 3.583820  [  448/ 1020]\n",
            "loss: 3.739179  [  512/ 1020]\n",
            "loss: 3.593465  [  576/ 1020]\n",
            "loss: 3.723087  [  640/ 1020]\n",
            "loss: 3.541700  [  704/ 1020]\n",
            "loss: 3.706108  [  768/ 1020]\n",
            "loss: 3.289244  [  832/ 1020]\n",
            "loss: 3.468252  [  896/ 1020]\n",
            "loss: 3.651670  [  960/ 1020]\n",
            "loss: 3.475576  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 11.6%, Avg loss: 3.792750 \n",
            "\n",
            "Epoch 7\n",
            "-------------------------------\n",
            "loss: 3.271826  [   64/ 1020]\n",
            "loss: 3.324409  [  128/ 1020]\n",
            "loss: 3.067522  [  192/ 1020]\n",
            "loss: 3.588733  [  256/ 1020]\n",
            "loss: 3.763381  [  320/ 1020]\n",
            "loss: 3.420867  [  384/ 1020]\n",
            "loss: 3.576232  [  448/ 1020]\n",
            "loss: 3.417882  [  512/ 1020]\n",
            "loss: 3.729626  [  576/ 1020]\n",
            "loss: 3.100338  [  640/ 1020]\n",
            "loss: 3.281252  [  704/ 1020]\n",
            "loss: 3.572854  [  768/ 1020]\n",
            "loss: 3.438211  [  832/ 1020]\n",
            "loss: 3.406047  [  896/ 1020]\n",
            "loss: 3.346339  [  960/ 1020]\n",
            "loss: 3.371715  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 12.8%, Avg loss: 3.730678 \n",
            "\n",
            "Epoch 8\n",
            "-------------------------------\n",
            "loss: 3.203040  [   64/ 1020]\n",
            "loss: 3.298651  [  128/ 1020]\n",
            "loss: 2.970955  [  192/ 1020]\n",
            "loss: 3.171139  [  256/ 1020]\n",
            "loss: 3.236941  [  320/ 1020]\n",
            "loss: 3.432387  [  384/ 1020]\n",
            "loss: 3.279188  [  448/ 1020]\n",
            "loss: 3.097183  [  512/ 1020]\n",
            "loss: 3.323667  [  576/ 1020]\n",
            "loss: 2.991359  [  640/ 1020]\n",
            "loss: 3.205487  [  704/ 1020]\n",
            "loss: 3.158354  [  768/ 1020]\n",
            "loss: 3.429135  [  832/ 1020]\n",
            "loss: 3.161773  [  896/ 1020]\n",
            "loss: 3.296157  [  960/ 1020]\n",
            "loss: 3.273264  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 11.1%, Avg loss: 3.754133 \n",
            "\n",
            "Epoch 9\n",
            "-------------------------------\n",
            "loss: 3.054316  [   64/ 1020]\n",
            "loss: 3.447783  [  128/ 1020]\n",
            "loss: 3.023690  [  192/ 1020]\n",
            "loss: 3.122750  [  256/ 1020]\n",
            "loss: 2.778419  [  320/ 1020]\n",
            "loss: 3.433455  [  384/ 1020]\n",
            "loss: 3.127782  [  448/ 1020]\n",
            "loss: 2.781643  [  512/ 1020]\n",
            "loss: 3.189882  [  576/ 1020]\n",
            "loss: 3.421232  [  640/ 1020]\n",
            "loss: 3.433451  [  704/ 1020]\n",
            "loss: 3.045162  [  768/ 1020]\n",
            "loss: 3.020763  [  832/ 1020]\n",
            "loss: 3.254053  [  896/ 1020]\n",
            "loss: 2.899350  [  960/ 1020]\n",
            "loss: 3.160902  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 11.8%, Avg loss: 3.674198 \n",
            "\n",
            "Epoch 10\n",
            "-------------------------------\n",
            "loss: 2.924665  [   64/ 1020]\n",
            "loss: 3.313212  [  128/ 1020]\n",
            "loss: 3.222837  [  192/ 1020]\n",
            "loss: 3.031037  [  256/ 1020]\n",
            "loss: 2.987183  [  320/ 1020]\n",
            "loss: 3.087553  [  384/ 1020]\n",
            "loss: 2.984802  [  448/ 1020]\n",
            "loss: 3.207606  [  512/ 1020]\n",
            "loss: 2.864890  [  576/ 1020]\n",
            "loss: 3.065902  [  640/ 1020]\n",
            "loss: 3.256155  [  704/ 1020]\n",
            "loss: 2.855028  [  768/ 1020]\n",
            "loss: 3.323278  [  832/ 1020]\n",
            "loss: 2.910934  [  896/ 1020]\n",
            "loss: 3.228875  [  960/ 1020]\n",
            "loss: 3.073490  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 13.9%, Avg loss: 3.660392 \n",
            "\n",
            "Epoch 11\n",
            "-------------------------------\n",
            "loss: 2.878083  [   64/ 1020]\n",
            "loss: 2.740902  [  128/ 1020]\n",
            "loss: 2.977140  [  192/ 1020]\n",
            "loss: 2.883583  [  256/ 1020]\n",
            "loss: 2.744335  [  320/ 1020]\n",
            "loss: 3.121415  [  384/ 1020]\n",
            "loss: 3.186160  [  448/ 1020]\n",
            "loss: 3.207206  [  512/ 1020]\n",
            "loss: 3.007918  [  576/ 1020]\n",
            "loss: 3.096728  [  640/ 1020]\n",
            "loss: 2.846639  [  704/ 1020]\n",
            "loss: 3.022103  [  768/ 1020]\n",
            "loss: 3.021922  [  832/ 1020]\n",
            "loss: 2.863347  [  896/ 1020]\n",
            "loss: 2.824257  [  960/ 1020]\n",
            "loss: 2.886134  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 16.4%, Avg loss: 3.552887 \n",
            "\n",
            "Epoch 12\n",
            "-------------------------------\n",
            "loss: 2.773262  [   64/ 1020]\n",
            "loss: 2.738429  [  128/ 1020]\n",
            "loss: 2.776853  [  192/ 1020]\n",
            "loss: 2.737517  [  256/ 1020]\n",
            "loss: 2.827824  [  320/ 1020]\n",
            "loss: 3.026915  [  384/ 1020]\n",
            "loss: 2.727858  [  448/ 1020]\n",
            "loss: 2.575504  [  512/ 1020]\n",
            "loss: 3.122644  [  576/ 1020]\n",
            "loss: 2.967795  [  640/ 1020]\n",
            "loss: 2.890203  [  704/ 1020]\n",
            "loss: 2.967837  [  768/ 1020]\n",
            "loss: 2.858590  [  832/ 1020]\n",
            "loss: 3.032464  [  896/ 1020]\n",
            "loss: 2.780797  [  960/ 1020]\n",
            "loss: 3.149400  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 16.4%, Avg loss: 3.589898 \n",
            "\n",
            "Epoch 13\n",
            "-------------------------------\n",
            "loss: 2.746082  [   64/ 1020]\n",
            "loss: 2.512729  [  128/ 1020]\n",
            "loss: 2.539983  [  192/ 1020]\n",
            "loss: 2.926421  [  256/ 1020]\n",
            "loss: 2.852407  [  320/ 1020]\n",
            "loss: 2.687730  [  384/ 1020]\n",
            "loss: 2.782969  [  448/ 1020]\n",
            "loss: 2.968773  [  512/ 1020]\n",
            "loss: 3.083792  [  576/ 1020]\n",
            "loss: 2.892533  [  640/ 1020]\n",
            "loss: 2.981612  [  704/ 1020]\n",
            "loss: 2.315774  [  768/ 1020]\n",
            "loss: 2.582653  [  832/ 1020]\n",
            "loss: 2.898922  [  896/ 1020]\n",
            "loss: 2.686836  [  960/ 1020]\n",
            "loss: 2.786178  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 17.5%, Avg loss: 3.533676 \n",
            "\n",
            "Epoch 14\n",
            "-------------------------------\n",
            "loss: 2.417359  [   64/ 1020]\n",
            "loss: 2.513927  [  128/ 1020]\n",
            "loss: 2.573246  [  192/ 1020]\n",
            "loss: 2.538634  [  256/ 1020]\n",
            "loss: 2.631786  [  320/ 1020]\n",
            "loss: 2.597079  [  384/ 1020]\n",
            "loss: 2.745196  [  448/ 1020]\n",
            "loss: 2.686636  [  512/ 1020]\n",
            "loss: 2.525145  [  576/ 1020]\n",
            "loss: 2.714288  [  640/ 1020]\n",
            "loss: 2.738154  [  704/ 1020]\n",
            "loss: 2.663337  [  768/ 1020]\n",
            "loss: 2.872217  [  832/ 1020]\n",
            "loss: 2.587554  [  896/ 1020]\n",
            "loss: 2.990675  [  960/ 1020]\n",
            "loss: 2.746748  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 16.3%, Avg loss: 3.600536 \n",
            "\n",
            "Epoch 15\n",
            "-------------------------------\n",
            "loss: 2.396601  [   64/ 1020]\n",
            "loss: 2.279924  [  128/ 1020]\n",
            "loss: 2.593644  [  192/ 1020]\n",
            "loss: 2.680103  [  256/ 1020]\n",
            "loss: 2.359772  [  320/ 1020]\n",
            "loss: 2.471452  [  384/ 1020]\n",
            "loss: 2.615324  [  448/ 1020]\n",
            "loss: 2.528352  [  512/ 1020]\n",
            "loss: 2.248989  [  576/ 1020]\n",
            "loss: 2.548028  [  640/ 1020]\n",
            "loss: 2.387017  [  704/ 1020]\n",
            "loss: 2.629457  [  768/ 1020]\n",
            "loss: 2.397567  [  832/ 1020]\n",
            "loss: 2.518015  [  896/ 1020]\n",
            "loss: 3.011918  [  960/ 1020]\n",
            "loss: 3.025731  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 19.4%, Avg loss: 3.453588 \n",
            "\n",
            "Epoch 16\n",
            "-------------------------------\n",
            "loss: 2.161769  [   64/ 1020]\n",
            "loss: 2.313761  [  128/ 1020]\n",
            "loss: 2.426172  [  192/ 1020]\n",
            "loss: 2.541541  [  256/ 1020]\n",
            "loss: 2.441230  [  320/ 1020]\n",
            "loss: 2.437231  [  384/ 1020]\n",
            "loss: 2.464879  [  448/ 1020]\n",
            "loss: 2.319028  [  512/ 1020]\n",
            "loss: 2.822512  [  576/ 1020]\n",
            "loss: 2.605555  [  640/ 1020]\n",
            "loss: 2.737488  [  704/ 1020]\n",
            "loss: 2.492944  [  768/ 1020]\n",
            "loss: 2.792886  [  832/ 1020]\n",
            "loss: 2.425807  [  896/ 1020]\n",
            "loss: 2.534510  [  960/ 1020]\n",
            "loss: 2.422199  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 19.3%, Avg loss: 3.446954 \n",
            "\n",
            "Epoch 17\n",
            "-------------------------------\n",
            "loss: 2.203674  [   64/ 1020]\n",
            "loss: 2.377704  [  128/ 1020]\n",
            "loss: 2.066749  [  192/ 1020]\n",
            "loss: 2.428301  [  256/ 1020]\n",
            "loss: 2.303493  [  320/ 1020]\n",
            "loss: 2.724399  [  384/ 1020]\n",
            "loss: 2.489807  [  448/ 1020]\n",
            "loss: 2.361610  [  512/ 1020]\n",
            "loss: 2.436270  [  576/ 1020]\n",
            "loss: 2.463287  [  640/ 1020]\n",
            "loss: 2.303804  [  704/ 1020]\n",
            "loss: 2.540255  [  768/ 1020]\n",
            "loss: 2.220352  [  832/ 1020]\n",
            "loss: 2.459878  [  896/ 1020]\n",
            "loss: 2.746691  [  960/ 1020]\n",
            "loss: 2.626865  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 19.9%, Avg loss: 3.497625 \n",
            "\n",
            "Epoch 18\n",
            "-------------------------------\n",
            "loss: 2.182696  [   64/ 1020]\n",
            "loss: 2.190843  [  128/ 1020]\n",
            "loss: 2.654083  [  192/ 1020]\n",
            "loss: 2.533699  [  256/ 1020]\n",
            "loss: 2.340958  [  320/ 1020]\n",
            "loss: 2.417718  [  384/ 1020]\n",
            "loss: 2.785807  [  448/ 1020]\n",
            "loss: 2.276462  [  512/ 1020]\n",
            "loss: 2.386235  [  576/ 1020]\n",
            "loss: 2.529249  [  640/ 1020]\n",
            "loss: 2.228932  [  704/ 1020]\n",
            "loss: 1.957039  [  768/ 1020]\n",
            "loss: 2.420077  [  832/ 1020]\n",
            "loss: 2.768734  [  896/ 1020]\n",
            "loss: 2.392431  [  960/ 1020]\n",
            "loss: 2.371119  [ 1020/ 1020]\n",
            "Test Error: \n",
            " Accuracy: 21.4%, Avg loss: 3.380273 \n",
            "\n",
            "Epoch 19\n",
            "-------------------------------\n",
            "loss: 2.417828  [   64/ 1020]\n",
            "loss: 2.193375  [  128/ 1020]\n",
            "loss: 2.073547  [  192/ 1020]\n",
            "loss: 2.045246  [  256/ 1020]\n",
            "loss: 2.178411  [  320/ 1020]\n",
            "loss: 2.134794  [  384/ 1020]\n",
            "loss: 2.268266  [  448/ 1020]\n",
            "loss: 2.244563  [  512/ 1020]\n",
            "loss: 2.232644  [  576/ 1020]\n",
            "loss: 2.726268  [  640/ 1020]\n",
            "loss: 2.203184  [  704/ 1020]\n",
            "loss: 2.325384  [  768/ 1020]\n",
            "loss: 2.610439  [  832/ 1020]\n",
            "loss: 2.259326  [  896/ 1020]\n",
            "loss: 2.484938  [  960/ 1020]\n",
            "loss: 2.294570  [ 1020/ 1020]\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start = time.time()\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 50\n",
        "for t in range(epochs):\n",
        "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
        "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    test_loop(test_dataloader, model, loss_fn)\n",
        "print(\"Done!\")\n",
        "\n",
        "end = time.time()\n",
        "total = end - start\n",
        "print(f\"Training took: {total/60} minutes!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}