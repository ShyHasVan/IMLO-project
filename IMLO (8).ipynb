{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iQl-2CRVuZWS",
        "outputId": "7e1bcc5e-4947-4b07-d398-5d50f1a31970"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "torch.Size([3, 224, 224])\n",
            "torch.Size([128, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda\n",
        "from torchvision import transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import time\n",
        "\n",
        "custom_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(degrees=30),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "training_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split= \"train\",\n",
        "    download=True,\n",
        "    transform= custom_transform,\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(training_data, batch_size=64, shuffle=True)\n",
        "\n",
        "val_data = datasets.Flowers102(\n",
        "    root=\"data\",\n",
        "    split= \"val\",\n",
        "    download=True,\n",
        "    transform= custom_transform,\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(val_data, batch_size=64, shuffle=True)\n",
        "\n",
        "device = (\n",
        "    \"cuda\"\n",
        "    if torch.cuda.is_available()\n",
        "    else \"mps\"\n",
        "    if torch.backends.mps.is_available()\n",
        "    else \"cpu\"\n",
        ")\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "count=0\n",
        "for i, (X_Train, y_train) in enumerate(training_data):\n",
        "  count+= 1\n",
        "print(X_Train.shape)\n",
        "conv1 = nn.Conv2d(3, 32, 3, 1, 1)\n",
        "conv2 = nn.Conv2d(32, 64, 3, 1, 1)\n",
        "conv3 = nn.Conv2d(64, 128, 3, 1, 1)\n",
        "n = F.relu(conv1(X_Train))\n",
        "\n",
        "n = F.max_pool2d(n, 2, 2)\n",
        "\n",
        "\n",
        "n = F.relu(conv2(n))\n",
        "\n",
        "n = F.max_pool2d(n, 2, 2)\n",
        "n = F.relu(conv3(n))\n",
        "\n",
        "n = F.max_pool2d(n, 2, 2)\n",
        "print(n.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M2bp2699ulxj"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, 3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(128 * 28 * 28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 102)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        #First Pass\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        #Second Pass\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        #Third Pass\n",
        "        x = F.relu(self.conv3(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "\n",
        "        x = x.view(-1, 128 * 28 * 28)\n",
        "        #Fully Connected Layers\n",
        "        x = self.dropout(F.relu(self.fc1(x)))\n",
        "        x = self.dropout(F.relu(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IeYBtuEtEdnD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07075f9e-ee35-464f-b662-3621f9962aa3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NeuralNetwork(\n",
              "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "  (fc1): Linear(in_features=100352, out_features=512, bias=True)\n",
              "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
              "  (fc3): Linear(in_features=256, out_features=102, bias=True)\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "\n",
        "model  =  NeuralNetwork()\n",
        "model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XOyGjfpDSc85"
      },
      "outputs": [],
      "source": [
        "# Training loop\n",
        "def train_loop(dataloader, model, loss_fn, optimizer):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for X, y in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X)\n",
        "        loss = loss_fn(outputs, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item() * X.size(0)\n",
        "    epoch_loss = running_loss / len(dataloader.dataset)\n",
        "    return epoch_loss\n",
        "\n",
        "# Validation loop\n",
        "def val_loop(dataloader, model, loss_fn):\n",
        "    model.eval()\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    val_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            pred = model(X)\n",
        "            val_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    val_loss /= num_batches\n",
        "    correct /= size\n",
        "    return val_loss, correct*100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG_ahimLFhhQ"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "SWZH7JsbPYvq",
        "outputId": "5a313f24-6551-478e-e43d-2a6a03ed0610"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "Train Loss: 4.7844, Val Loss: 4.6239, Val Accuracy: 0.78\n",
            "Epoch 2/100\n",
            "Train Loss: 4.6209, Val Loss: 4.5999, Val Accuracy: 2.06\n",
            "Epoch 3/100\n",
            "Train Loss: 4.5605, Val Loss: 4.4312, Val Accuracy: 2.16\n",
            "Epoch 4/100\n",
            "Train Loss: 4.4275, Val Loss: 4.2420, Val Accuracy: 3.82\n",
            "Epoch 5/100\n",
            "Train Loss: 4.3048, Val Loss: 4.1773, Val Accuracy: 3.43\n",
            "Epoch 6/100\n",
            "Train Loss: 4.1684, Val Loss: 4.0758, Val Accuracy: 3.73\n",
            "Epoch 7/100\n",
            "Train Loss: 4.1655, Val Loss: 4.0357, Val Accuracy: 5.10\n",
            "Epoch 8/100\n",
            "Train Loss: 4.0377, Val Loss: 3.9697, Val Accuracy: 6.67\n",
            "Epoch 9/100\n",
            "Train Loss: 4.0005, Val Loss: 3.8680, Val Accuracy: 7.35\n",
            "Epoch 10/100\n",
            "Train Loss: 3.9468, Val Loss: 3.8578, Val Accuracy: 9.51\n",
            "Epoch 11/100\n",
            "Train Loss: 3.8922, Val Loss: 3.8422, Val Accuracy: 7.94\n",
            "Epoch 12/100\n",
            "Train Loss: 3.8740, Val Loss: 3.7842, Val Accuracy: 8.63\n",
            "Epoch 13/100\n",
            "Train Loss: 3.7594, Val Loss: 3.7461, Val Accuracy: 9.61\n",
            "Epoch 14/100\n",
            "Train Loss: 3.6990, Val Loss: 3.7314, Val Accuracy: 11.18\n",
            "Epoch 15/100\n",
            "Train Loss: 3.6822, Val Loss: 3.6554, Val Accuracy: 13.82\n",
            "Epoch 16/100\n",
            "Train Loss: 3.6106, Val Loss: 3.6115, Val Accuracy: 11.08\n",
            "Epoch 17/100\n",
            "Train Loss: 3.5516, Val Loss: 3.6005, Val Accuracy: 12.65\n",
            "Epoch 18/100\n",
            "Train Loss: 3.4728, Val Loss: 3.5661, Val Accuracy: 13.33\n",
            "Epoch 19/100\n",
            "Train Loss: 3.5395, Val Loss: 3.5616, Val Accuracy: 13.53\n",
            "Epoch 20/100\n",
            "Train Loss: 3.4009, Val Loss: 3.5125, Val Accuracy: 15.00\n",
            "Epoch 21/100\n",
            "Train Loss: 3.3745, Val Loss: 3.4834, Val Accuracy: 15.10\n",
            "Epoch 22/100\n",
            "Train Loss: 3.3393, Val Loss: 3.5078, Val Accuracy: 16.47\n",
            "Epoch 23/100\n",
            "Train Loss: 3.2840, Val Loss: 3.4723, Val Accuracy: 14.90\n",
            "Epoch 24/100\n",
            "Train Loss: 3.3090, Val Loss: 3.4794, Val Accuracy: 15.00\n",
            "Epoch 25/100\n",
            "Train Loss: 3.2648, Val Loss: 3.4900, Val Accuracy: 15.20\n",
            "Epoch 26/100\n",
            "Train Loss: 3.1889, Val Loss: 3.5211, Val Accuracy: 16.27\n",
            "Epoch 27/100\n",
            "Train Loss: 3.2033, Val Loss: 3.4591, Val Accuracy: 16.18\n",
            "Epoch 28/100\n",
            "Train Loss: 3.2253, Val Loss: 3.4372, Val Accuracy: 16.76\n",
            "Epoch 29/100\n",
            "Train Loss: 3.1665, Val Loss: 3.4495, Val Accuracy: 15.78\n",
            "Epoch 30/100\n",
            "Train Loss: 3.1064, Val Loss: 3.4355, Val Accuracy: 14.71\n",
            "Epoch 31/100\n",
            "Train Loss: 2.9895, Val Loss: 3.4599, Val Accuracy: 16.96\n",
            "Epoch 32/100\n",
            "Train Loss: 3.0521, Val Loss: 3.4114, Val Accuracy: 18.63\n",
            "Epoch 33/100\n",
            "Train Loss: 3.0193, Val Loss: 3.4211, Val Accuracy: 16.76\n",
            "Epoch 34/100\n",
            "Train Loss: 3.0034, Val Loss: 3.3957, Val Accuracy: 17.45\n",
            "Epoch 35/100\n",
            "Train Loss: 3.0438, Val Loss: 3.3391, Val Accuracy: 18.63\n",
            "Epoch 36/100\n",
            "Train Loss: 2.9344, Val Loss: 3.4410, Val Accuracy: 16.96\n",
            "Epoch 37/100\n",
            "Train Loss: 2.8585, Val Loss: 3.4353, Val Accuracy: 18.82\n",
            "Epoch 38/100\n",
            "Train Loss: 2.9135, Val Loss: 3.4114, Val Accuracy: 17.75\n",
            "Epoch 39/100\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-8063e2face10>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-a43197046d26>\u001b[0m in \u001b[0;36mtrain_loop\u001b[0;34m(dataloader, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             )\n\u001b[0;32m--> 522\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    267\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Training\n",
        "start = time.time()\n",
        "epochs = 100\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    train_loss = train_loop(train_dataloader, model, loss_fn, optimizer)\n",
        "    val_loss, val_accuracy = val_loop(val_dataloader, model, loss_fn)\n",
        "    print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.2f}\")\n",
        "    scheduler.step(val_loss)  # Adjust learning rate based on validation loss\n",
        "end = time.time()\n",
        "print(f\"Training took: {(end - start) / 60:.2f} minutes!\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "vp": {
      "vp_note_display": false,
      "vp_note_width": 0,
      "vp_section_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}